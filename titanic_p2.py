# -*- coding: utf-8 -*-
"""titanic-P2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/197Am5vpPrd9tt0Hihhpt-OEPIEaswoNB

# Bibliotecas
"""

!pip install fancyimpute

from fancyimpute import KNN
from sklearn.impute import KNNImputer
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

"""# Leitura dos dados

Carregamos nosso dataset e exibimos as primeiras linhas para ter uma visão inicial dos dados.
"""

df = pd.read_csv('/content/train.csv')
#df = pd.read_csv("/content/drive/MyDrive/Disciplinas/Sistemas Inteligentes/train.csv", delimiter=';')

df.head(90)

df["Survived"].value_counts()

df.dtypes

"""
Observamos que algumas features estão como "object", podemos mudá-las para "category", para que sejam armazenadas de mais eficiente, além de facilitar também operações sobre elas. Além disso, a primeira letra da cabine refere-se ao pavimento da cabine do hóspede. Por isso, removemos os números da coluna em questão para conseguir utilizar o dado de uma forma mais apropriada."""

df["Cabin"] = df["Cabin"].str[0]

df["Sex"] = df["Sex"].astype("category")
df["Cabin"] = df["Cabin"].astype("category")
df["Embarked"] = df["Embarked"].astype("category")

df["Cabin"].unique()

df.dtypes

"""# Verificar a existencia de dados duplicados"""

len(df[df.duplicated()])

df.drop_duplicates(keep='first', inplace=True)
len(df[df.duplicated()])

"""#Relevância da variável cabine

Considerando que a variável "Cabine" possui muitos valores nulos, irá ser realizada uma análise que terá como objetivo avaliar se essa variável possa gerar algum impacto relevante no classificador final ou se desconsiderá-la possa ser viável.
"""

mat = []

size = df[(df["Cabin"].isnull()) & (df["Survived"] == 1)].shape[0]
sizetotal = df[df["Cabin"].isnull()].shape[0]
mat.append([size, round(size*100/sizetotal, 2), sizetotal])

size = df[(df["Cabin"].notnull()) & (df["Survived"] == 1)].shape[0]
sizetotal = df[df["Cabin"].notnull()].shape[0]
mat.append([size, round(size*100/sizetotal, 2), sizetotal])

Xaxis=["Survived", "Survival Percentage", "Total"]
Yaxis=["Cabin - NaN", "Cabin - Not Null"]
plt.matshow(mat, cmap=plt.cm.Blues)
for i in range(len(mat)):
    for j in range(len(mat[i])):
        c = mat[i][j]
        plt.text(j, i, str(c), va='center', ha='center', color = '#FF0000')

plt.xticks(np.arange(0,len(Xaxis), step = 1), labels = [Xaxis[i] for i in range(len(Xaxis))])
plt.yticks(np.arange(0,len(Yaxis), step = 1), labels = [Yaxis[i] for i in range(len(Yaxis))])

"""Observando as ocorências dessa variável, podemos observar que talvez ela possa ser importante para o classificador, pois passageiros com registro nulo de cabine possuem baixa chance de sobrevivência e passageiros com registro possui uma chance de sobrevivência mais elevada. Logo, pode-se concluir que incluir essa variável em pelo menos uma ocorrência de classificador talvez possa ser capaz de gerar resultados positivos.

# Tratamento de valores ausentes

Para os dados numéricos, utilizaremos a interpolação  para prever os valores ausentes com base nos valores vizinhos. Para os dados categórigos, optamos por colocar o atributo em questão como "desconhecido".
"""

df.isnull().sum()

predict_df = df.copy(deep=True)
predict_df['Age'].interpolate(method='linear', inplace=True)

predict_df.isnull().sum()

predict_df['DadosNulos'] = df['Cabin'].isnull().astype(int)
predict_df['Cabin'] = predict_df['Cabin'].cat.add_categories(['Desconhecido'])
predict_df['Embarked'] = predict_df['Embarked'].cat.add_categories(['Desconhecido'])

predict_df['Cabin'].fillna('Desconhecido', inplace=True)
predict_df['Embarked'].fillna('Desconhecido', inplace=True)
predict_df

predict_df.isnull().sum()

"""# Graficos

Para analisar o comportamento dos dados, e possivelmente indentificar as categorias que podem estar mais ou menos relacionadas ao resultado final (identificar os passageiros que sobreviveram ou não ao acidente do Titanic), a plotagem de algumas distribuições e a subsequente visualização de padrões presentes nos dados será realizada. O começo dessa análise irá partir com base na porcentagem total de sobreviventes:
"""

predict_df['Survived'].value_counts()

plt.figure();

pltdf = predict_df['Survived'].value_counts()

pltdf.plot.bar(y = [pltdf[0], pltdf[1]], figsize=(5, 5), grid = True)

plt.title('Número de sobrevientes e não sobreviventes')
plt.xlabel('Não Sobreviventes / Sobreviventes')
plt.ylabel('Quantidade')
plt.show()

percentage = pltdf[1] * 100 / (pltdf[0]+pltdf[1])
print("Porcentagem de sobreviventes:")
print(str(percentage) + "%")

"""Logo, temos que, a porcentagem de uma pessoa qualquer ter sobrevivido ao acidente, sem considerar questões como idade ou classe, é de aproximadamente 39%. Para observar como os dados que temos acesso podem nos ajudar a inferir a chance da sobrevivência de uma pessoa, uma análise acerca da distribuição de idade de sobreviventes e não sobreviventes será realizada a seguir:"""

def string_labels (survivor_labels):
  string_labels = []
  for label in survivor_labels:
    if(label == 0): string_labels.append("Não Sobreviventes")
    else: string_labels.append("Sobreviventes")
  return string_labels

pltdf = predict_df.copy()

tmp_df = pd.DataFrame({"Survived": string_labels(pltdf["Survived"]), "age": pltdf["Age"]})

tmp_df.plot.hist(column=["age"], by="Survived", figsize=(4, 6), grid = True)

"""Analisando o histograma, podemos perceber uma porcentagem relativa maior de sobreviventes menores de idade e uma porcentagem bem menor em sujeitos acima dos 60 anos de idade. Continuando a análise da chance de sobrevivência baseada na idade:"""

tmp_df["age"].max()

mat = []
age_gaps = [-1, 10, 20, 30, 40, 50, 60, 70, 80]
for i in range (len(age_gaps)-1):
  size = pltdf[(pltdf["Age"] > age_gaps[i]) & (pltdf["Age"] <= age_gaps[i+1]) & (pltdf["Survived"] == 1)].shape[0]
  sizetotal = pltdf[(pltdf["Age"] > age_gaps[i]) & (pltdf["Age"] <= age_gaps[i+1])].shape[0]
  mat.append([size, round(size*100/sizetotal, 2), sizetotal])

Xaxis=["Survived", "Survival Percentage", "Total"]
Yaxis=["0-10", "11-20", "21-30", "31-40", "41-50", "51-60", "61-70", "71-80"]
plt.matshow(mat, cmap=plt.cm.Blues)
for i in range(len(mat)):
    for j in range(len(mat[i])):
        c = mat[i][j]
        plt.text(j, i, str(c), va='center', ha='center', color = '#FF0000')

plt.xticks(np.arange(0,len(Xaxis), step = 1), labels = [Xaxis[i] for i in range(len(Xaxis))])
plt.yticks(np.arange(0,len(Yaxis), step = 1), labels = [Yaxis[i] for i in range(len(Yaxis))])

"""Com isso, é possível confirmar as suspeitas anteriores, já que realmente pode ser observado um maior índice de sobrevivência para pessoas abaixo de 10 anos, uma menor para sujeitos acima de 60, e uma taxa sem variações muito bruscas para as outras idades. Extendendo essa análise para sexo e classe:"""

mat = []
size = pltdf[(pltdf["Sex"] == "male") & (pltdf["Survived"] == 1)].shape[0]
sizetotal = pltdf[(pltdf["Sex"] == "male")].shape[0]
mat.append([size, round(size*100/sizetotal, 2), sizetotal])
size = pltdf[(pltdf["Sex"] == "female") & (pltdf["Survived"] == 1)].shape[0]
sizetotal = pltdf[(pltdf["Sex"] == "female")].shape[0]
mat.append([size, round(size*100/sizetotal, 2), sizetotal])

Xaxis=["Survived", "Survival Percentage", "Total"]
Yaxis=["Male", "Female"]
plt.matshow(mat, cmap=plt.cm.Blues)
for i in range(len(mat)):
    for j in range(len(mat[i])):
        c = mat[i][j]
        plt.text(j, i, str(c), va='center', ha='center', color = '#FF0000')

plt.xticks(np.arange(0,len(Xaxis), step = 1), labels = [Xaxis[i] for i in range(len(Xaxis))])
plt.yticks(np.arange(0,len(Yaxis), step = 1), labels = [Yaxis[i] for i in range(len(Yaxis))])

mat = []
pclasses = [1, 2, 3]
for pclass in pclasses :
  size = pltdf[(pltdf["Pclass"] == pclass) & (pltdf["Survived"] == 1)].shape[0]
  sizetotal = pltdf[(pltdf["Pclass"] == pclass)].shape[0]
  mat.append([size, round(size*100/sizetotal, 2), sizetotal])

Xaxis=["Survived", "Survival Percentage", "Total"]
Yaxis=["Upper", "Middle", "Lower"]
plt.matshow(mat, cmap=plt.cm.Blues)
for i in range(len(mat)):
    for j in range(len(mat[i])):
        c = mat[i][j]
        plt.text(j, i, str(c), va='center', ha='center', color = '#FF0000')

plt.xticks(np.arange(0,len(Xaxis), step = 1), labels = [Xaxis[i] for i in range(len(Xaxis))])
plt.yticks(np.arange(0,len(Yaxis), step = 1), labels = [Yaxis[i] for i in range(len(Yaxis))])

"""Logo, também podemos analisar uma certa influência da classe do ticket e do sexo que apontam para maiores chances de sobrevivência. Por último, também será realizada a análise do número de parentes à bordo, que intuitivamente parece ser menos sujestivo que os outros dados análisados até agora:"""

pltdf["Parch"].max()

mat = []
parchs = [0, 1, 2, 3, 4, 5, 6,]
for parch in parchs :
  size = pltdf[(pltdf["Parch"] == parch) & (pltdf["Survived"] == 1)].shape[0]
  sizetotal = pltdf[(pltdf["Parch"] == parch)].shape[0]
  if(sizetotal > 0): mat.append([size, round(size*100/sizetotal, 2), sizetotal])

Xaxis=["Survived", "Survival Percentage", "Total"]
Yaxis=[0, 1, 2, 3, 4, 5, 6]
plt.matshow(mat, cmap=plt.cm.Blues)
for i in range(len(mat)):
    for j in range(len(mat[i])):
        c = mat[i][j]
        plt.text(j, i, str(c), va='center', ha='center', color = '#FF0000')

plt.xticks(np.arange(0,len(Xaxis), step = 1), labels = [Xaxis[i] for i in range(len(Xaxis))])
plt.yticks(np.arange(0,len(Yaxis), step = 1), labels = [Yaxis[i] for i in range(len(Yaxis))])

"""Boa parte dos valores encontrados possuem amostras muito pequenas, o que pode reduzir a aplicabilidade desse dado, porém, ainda assim, pode-se observar uma chance consideravelmente maior de sobrevivência de pessoas com 1 ou 2 parentes ou crianças à bordo do que aqueles sem parentes, ou seja, esse dado ainda pode também ser considerado relevante de forma e ser utilizado como um dos parâmetros do modelo.

#Discretização
"""

bins = [0, 10, 50, 60, float('inf')]
labels = ['0-10', '11-50', '51-60', '60+']

predict_df['Age_disc'] = pd.cut(predict_df['Age'], bins=bins, labels=labels, right=False)

predict_df

"""# Normalização

A normalização é crucial para garantir que variáveis com escalas diferentes não distorçam os resultados analíticos, especialmente em técnicas de análise que são sensíveis a variações de escala, como modelagem preditiva e algoritmos de machine learning.
"""

predict_norm_df = predict_df.copy(deep=True)

predict_norm_df

predict_norm_df['Sex'] = predict_norm_df['Sex'].astype('category').cat.codes
predict_norm_df['Cabin'] = predict_norm_df['Cabin'].astype('category').cat.codes
predict_norm_df['Age_disc'] = predict_norm_df['Age_disc'].astype('category').cat.codes

predict_norm_df['Age'] = (predict_norm_df['Age'] - predict_norm_df['Age'].min()) / (predict_norm_df['Age'].max() - predict_norm_df['Age'].min())
predict_norm_df['Pclass'] = (predict_norm_df['Pclass'] - predict_norm_df['Pclass'].min()) / (predict_norm_df['Pclass'].max() - predict_norm_df['Pclass'].min())
predict_norm_df['SibSp'] = (predict_norm_df['SibSp'] - predict_norm_df['SibSp'].min()) / (predict_norm_df['SibSp'].max() - predict_norm_df['SibSp'].min())
predict_norm_df['Parch'] = (predict_norm_df['Parch'] - predict_norm_df['Parch'].min()) / (predict_norm_df['Parch'].max() - predict_norm_df['Parch'].min())
predict_norm_df['Fare'] = (predict_norm_df['Fare'] - predict_norm_df['Fare'].min()) / (predict_norm_df['Fare'].max() - predict_norm_df['Fare'].min())
predict_norm_df['Cabin'] = (predict_norm_df['Cabin'] - predict_norm_df['Cabin'].min()) / (predict_norm_df['Cabin'].max() - predict_norm_df['Cabin'].min())
predict_norm_df['Age_disc'] = (predict_norm_df['Age_disc'] - predict_norm_df['Age_disc'].min()) / (predict_norm_df['Age_disc'].max() - predict_norm_df['Age_disc'].min())

predict_norm_df.head(10)

"""Normalizar os dados é importante porque algoritmos como K-Means clustering, PCA, regressão linear e redes neurais podem ser influenciados negativamente por variáveis que têm escalas muito diferentes. Isso pode levar a resultados distorcidos ou subótimos. Além de melhorar a precisão dos modelos, a normalização facilita a visualização dos dados, ajudando a identificar padrões e outliers de forma mais clara e precisa.

#Divisão do dataset e balanceamento das classes
"""

def div_test_train(df):
  X = df.drop('Survived', axis=1)
  y = df['Survived']

  # Dividir o dataset em treinamento e teste
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

  # Aplicar SMOTE ao conjunto de treinamento
  smote = SMOTE(random_state=42)
  X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

  return X_train_res, X_test, y_train_res, y_test

df_div = predict_norm_df.copy(deep=True)
df_div.drop(["Name", "PassengerId", "Ticket", "Embarked", "Age"], axis=1, inplace=True)

"""#Arvore de decisão

Árvores de decisão são um modelo de aprendizado de máquina que toma decisões com base em uma série de perguntas simples, organizadas em forma de árvore. Cada "nó" da árvore representa uma decisão sobre uma característica dos dados, e cada "ramo" representa o resultado dessa decisão, direcionando para o próximo nó ou para uma decisão final.

O processo começa com o nó raiz, onde a primeira decisão é feita. A árvore é dividida em ramos com base nos valores das características, e cada ramificação leva a novos nós onde outras decisões são feitas. Esse processo continua até que as folhas da árvore sejam alcançadas, que representam a decisão final ou a previsão.

## Teste 1
"""

df_teste1 = df_div.copy(deep=True)
X_train, X_test, y_train, y_test = div_test_train(df_teste1)

clf = DecisionTreeClassifier(random_state=42)

# Treinar o modelo
clf.fit(X_train, y_train)

# Fazer previsões no conjunto de teste
y_pred = clf.predict(X_test)

# Avaliar o modelo
accuracy = accuracy_score(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Acurácia:", accuracy)
print("\nRelatório de Classificação:\n", class_report)
print("\nMatriz de Confusão:\n", conf_matrix)

plot_tree(
    clf,
    filled=True,
    feature_names=X_train.columns,
    class_names=['Not Survived', 'Survived'],
    rounded=True,
    proportion=False,
    fontsize=10
)

importances = clf.feature_importances_
indices = importances.argsort()[::-1]  # Ordena as importâncias em ordem decrescente

# Exibir a importância das features
print("\nImportância das Features:")
for i in indices:
    print(f"{X_train.columns[i]}: {importances[i]:.4f}")

# Visualizar a importância das features em um gráfico
plt.figure(figsize=(10,6))
plt.title("Importância das Features")
plt.barh(range(X_train.shape[1]), importances[indices], align='center')
plt.yticks(range(X_train.shape[1]), [X_train.columns[i] for i in indices])
plt.xlabel("Importância")
plt.show()

"""##Teste 2"""

df_teste2 = df_div.copy(deep=True)
df_teste2.drop(["Cabin", "Parch", "SibSp", "DadosNulos"], axis=1, inplace=True)
X_train, X_test, y_train, y_test = div_test_train(df_teste2)

clf = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_split=5, random_state=42)

# Treinar o modelo
clf.fit(X_train, y_train)

# Fazer previsões no conjunto de teste
y_pred = clf.predict(X_test)

# Avaliar o modelo
accuracy = accuracy_score(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Acurácia:", accuracy)
print("\nRelatório de Classificação:\n", class_report)
print("\nMatriz de Confusão:\n", conf_matrix)
plt.figure(figsize=(20,10))

plot_tree(
    clf,
    filled=True,
    feature_names=X_train.columns,
    class_names=['Not Survived', 'Survived'],
    rounded=True,
    proportion=False,
    fontsize=10
)
plt.show()

importances = clf.feature_importances_
indices = importances.argsort()[::-1]  # Ordena as importâncias em ordem decrescente

# Exibir a importância das features
print("\nImportância das Features:")
for i in indices:
    print(f"{X_train.columns[i]}: {importances[i]:.4f}")

# Visualizar a importância das features em um gráfico
plt.figure(figsize=(10,6))
plt.title("Importância das Features")
plt.barh(range(X_train.shape[1]), importances[indices], align='center')
plt.yticks(range(X_train.shape[1]), [X_train.columns[i] for i in indices])
plt.xlabel("Importância")
plt.show()

"""##Teste 3"""

df_teste3 = df_div.copy(deep=True)
df_teste3.drop(["Cabin", "Parch", "SibSp", "Cabin", "DadosNulos"], axis=1, inplace=True)
X_train, X_test, y_train, y_test = div_test_train(df_teste3)

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5, 10],
    'max_features': [None, 'sqrt', 'log2']
}

clf = DecisionTreeClassifier(random_state=42)

grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Melhores parâmetros:", best_params)

best_clf = grid_search.best_estimator_
best_clf.fit(X_train, y_train)

y_pred = best_clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Acurácia:", accuracy)
print("\nRelatório de Classificação:\n", class_report)
print("\nMatriz de Confusão:\n", conf_matrix)

plt.figure(figsize=(20,10))
plot_tree(best_clf, feature_names=X_train.columns, class_names=['Not Survived', 'Survived'], filled=True, rounded=True)
plt.show()

"""Mesmo que a acurácia não tenha melhorado significativamente com o uso do GridSearchCV, o valor obtido é mais confiável porque leva em conta a validação cruzada. A validação cruzada envolve dividir o conjunto de dados em vários subconjuntos e treinar e testar o modelo em diferentes combinações desses subconjuntos. Isso significa que o modelo é avaliado em vários conjuntos de dados diferentes, o que proporciona uma estimativa mais robusta do seu desempenho real.

Portanto, mesmo que a acurácia final não tenha mudado muito, o fato de ter sido calculada usando validação cruzada indica que a avaliação é mais geral e menos dependente de uma única divisão dos dados. Em outras palavras, o modelo foi testado em várias configurações diferentes do conjunto de dados, o que ajuda a garantir que a acurácia medida é uma estimativa mais precisa e representativa do desempenho do modelo em dados novos e não vistos.

#Random Forest

O Random Forest é um modelo de aprendizado de máquina que utiliza um conjunto de árvores de decisão para melhorar a precisão das previsões e reduzir o risco de overfitting. Em vez de usar uma única árvore, o Random Forest constrói várias árvores de decisão, cada uma treinada em um subconjunto diferente dos dados. Esses subconjuntos são selecionados aleatoriamente com substituição, o que garante diversidade entre as árvores.

GridSearchCV é uma técnica essencial para otimizar modelos de machine learning, como o RandomForestClassifier. Seu principal objetivo é encontrar a melhor combinação de hiperparâmetros para o modelo, o que pode resultar em um desempenho significativamente melhor.

Os hiperparâmetros são configurações do modelo que não são aprendidas durante o treinamento, mas definidas antes. No caso do Random Forest, exemplos incluem o número de árvores (n_estimators), a profundidade máxima das árvores (max_depth), e o número mínimo de amostras por folha (min_samples_leaf). Ajustar esses parâmetros pode ser crucial, pois configurações inadequadas podem levar a problemas como overfitting ou underfitting.
"""

df_teste4 = df_div.copy(deep=True)
df_teste4.drop(["Cabin", "Parch", "SibSp", "Cabin", "DadosNulos"], axis=1, inplace=True)
X_train, X_test, y_train, y_test = div_test_train(df_teste4)

rf = RandomForestClassifier(random_state=42)

# Definir a grade de parâmetros para a busca
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Inicializar o GridSearchCV com 5-fold cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Ajustar o modelo
grid_search.fit(X_train, y_train)

# Melhor conjunto de parâmetros
best_params = grid_search.best_params_
print("Melhores parâmetros:", best_params)

best_rf = grid_search.best_estimator_
best_rf.fit(X_train, y_train)

# Fazer previsões no conjunto de teste
y_pred = best_rf.predict(X_test)

# Avaliar o modelo
accuracy = accuracy_score(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Acurácia:", accuracy)
print("\nRelatório de Classificação:\n", class_report)
print("\nMatriz de Confusão:\n", conf_matrix)

# Verificar as importâncias das features
importances = best_rf.feature_importances_
indices = importances.argsort()[::-1]  # Ordena as importâncias em ordem decrescente

# Exibir a importância das features
print("\nImportância das Features:")
for i in indices:
    print(f"{X_train.columns[i]}: {importances[i]:.4f}")

# Visualizar a importância das features em um gráfico
plt.figure(figsize=(10,6))
plt.title("Importância das Features")
plt.barh(range(X_train.shape[1]), importances[indices], align='center')
plt.yticks(range(X_train.shape[1]), [X_train.columns[i] for i in indices])
plt.xlabel("Importância")
plt.show()